{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The receptive field of a unit (neuron) in a CNN is the spatial region of the input that has an affect on the unit. The receptive field of a unit that is not in the first hidden layer is the union of the recptive field of it's input, because it is affected by it. The higher the convolution kernel size, stride and dilation of a layer, the larger the receptive field is.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "a. *Kernel Size* - The size of the convolution kernel. At each layer, each unit is affected by the amount of units is that depended on how large the kernel size is. Thus, the larger value of kernel size we use, the higher the rate in which the receptive field grows.\n",
    "The higher the kernel size is, the more features of the input affect each unit of the output, but the extracted features are less regional.\n",
    "\n",
    "\n",
    "b. *Stride* - Determines how many input points are skipped between each step of the convolution. This value, has an affect on how an output unit will be similar to it's neighbours becasue they have common inputs. When this value is higher, neighbor units will be depended on different inputs, thus when performing convolution again on the output, the next layer will have a higher receptive field.\n",
    "The stride value, does not affect directly the receptive field of a layer but only the receptive field of the next layer.\n",
    "\n",
    "c. *Dilation* - Determines the gap in between elements in the convolution kernel for each step.  The higher the dilation, the larger the area of the input the kernel \"looks at\", but more input values are ignored between those who are used. Thats means of course that increasing the dilation also increases the receptive field, but some features from the input are lost. A dilation may be similar to pooling + convolution.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:16.753322Z",
     "iopub.status.busy": "2021-01-26T09:18:16.752610Z",
     "iopub.status.idle": "2021-01-26T09:18:17.987501Z",
     "shell.execute_reply": "2021-01-26T09:18:17.988034Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Let $r_k$ be the receptive field a single neuron at the k'th layer, $g_k$ the kernel size for layer k, $s_k$ the stride at layer k.\n",
    "We can think of the dilated conv layer as a layer with size 13.\n",
    "MaxPooling2d(2) as defined in pytorch can be thought as Conv2d with kernel size = 2 , stride = 2, padding =0, dilation=1.\n",
    "Relu activation does not affect on the receptive field at all.\n",
    "\n",
    "$r_k = r_{k-1} + (g_k -1)*\\Pi_{i=1}^{k-1} s_i$\n",
    "\n",
    "Using the equation we get\n",
    "\n",
    "$input : r_0 = 1 $\n",
    "\n",
    "$conv: r_1 = 3 $\n",
    "\n",
    "$MaxPooling: r_2 = 3 + (2 - 1) * 1 = 4 $\n",
    "\n",
    "$conv: r_3 = 4 + (5-1) * 1 * 2 = 12 $\n",
    "\n",
    "$MaxPooling: r_4 = 12 + (2-1) * 1 * 2 * 2 = 16 $\n",
    "\n",
    "$conv: r_5 = 16 + (13-1) * 1 *2 *2 *2 = 112 $\n",
    "\n",
    "In conclusion , the receptive field of the last layer is 122x122 square.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "    \n",
    "By creating residual connection in the network, we force the layer to learn the identity function plus another\n",
    "function of the input in contrast to a regular layer where the network learns a completly new function of the input.\n",
    "Therefore, with residual connection the network is expected to learn different mapping, which will probably have smaller weights since it's\n",
    "initial pretrained output is the input + random function, therefore we will get completly different filters.\n",
    "In order to achieve the same out put with the new architecture, the new one will have to learn parameters relative to the input.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A value is dropped out if it was dropped out in the first dropout layer, the second one, or both.\n",
    "We will calculate the probability of a value for not being dropped out:\n",
    "\n",
    "$ 1 - q = (1-p_1)(1-p_2) $\n",
    "\n",
    "$ q = 1 - (1 - p_2 - p_1 + p_1 p_2) = p_1 + p_2 - p_1 p_2$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Although as a rule of thumb a dropout is suggested to be put after the activation layer,\n",
    "it can also be put before it, and in the case of a relu it is even more computationally efficient.\n",
    "lets take a look at ReLU.\n",
    "\n",
    "for example:\n",
    "ReLU - > Dropout\n",
    "1. x<0 and chosen -  y = 0\n",
    "2. x<0 and not chosen  - y = 0\n",
    "3. x>=0 and not chosen  - y = 0\n",
    "4. x>=0 and chosen - y = x\n",
    "\n",
    "Dropout - > ReLU\n",
    "1. x<0 and chosen -  y = 0\n",
    "2. x<0 and not chosen  - y = 0\n",
    "3. x>=0 and not chosen  - y = 0\n",
    "4. x>=0 and chosen - y = x\n",
    "\n",
    "All the cases are equal thus the order doesnt matter.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Consider an activation vector $\\mathbf{x}$, and the same activation vector $\\mathbf{\\tilde x}$ with dropout applied, which means each \n",
    "\n",
    "$\\mathbf{\\tilde x_i} = \\begin{cases}\n",
    "    x_i & \\text{w.p } (1-p) \\\\ \n",
    "    0 & \\text{w.p } (p)\n",
    "\\end{cases}$ \n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\mathbb{E}[\\mathbf{\\tilde x}] = 1/n \\sum_{i=1}^{n}{\\tilde x_i} = 1/n \\sum_{i=1}^{n}{(1-p)x_i} = (1-p) \\mathbb{E}[\\mathbf{x}]$ \n",
    "\n",
    "The expectation of the activation is :\n",
    "$\\mathbb{E}[\\sum{w_i \\tilde x_i}] = (1-p) \\mathbb{E}[\\sum{w_i x_i}]$ therefore in order to maintain the expectation, we have to scale it by $1/(1-p)$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We would not train this model with L2 loss since it is a classification task. L2 is more suitable for a regression task while there are better loss function for classification such as cross entropy loss or logistic loss.\n",
    "\n",
    "For example Cross entropy loss is better because it penalizes the model in cases of uncertainty, that means that even if the model predicted the correct label, but with a probability far from 1, it will be penalized and the model will be trained to predict with higher certainty.\n",
    "\n",
    "for example, in the case of a hot dog and an output of 0.55 from the classifier, which means that the model predicted 55% hotdog, L2 loss for that sample will be:\n",
    "\n",
    "$l(0.55,1) = (1-0.55)^2 = 0.2025$\n",
    "\n",
    "and cross entropy loss wil be:\n",
    "\n",
    "$l(0.55,1) = -log(0.55) = 0.59$\n",
    "\n",
    "which is higher, as it should be because the model is uncertain.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Since the network is quite deep, with a sigmoid activation and without batch normalization or skip connections, the sigmoids could easily squish great information to a very small range\n",
    "and that way cause losing much information along the net .Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.\n",
    " We assume the reason is vanishing gradients, the gradients gets close to zero and the model stops training.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The gradient of tanh, just like sigmoid, gets close to zero rapidly when the input gets far from zero. tanh's derivative - sech - lives in (0,1) range as sigmoid.\n",
    "therefore we don't expect this solution to work.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "- False - Activation functions are not the only possible reason for vanishing gradients.\n",
    "vanishing gradients could be a result of too deep network without skip connections as well.\n",
    "- False - the gradient of ReLU is constant w.r.t input when the input is positive.\n",
    "- True - ReLU can cause dead neurons for values smaller then 0 and thus gradient 0 as well, that could be an advantage in same cases\n",
    "that will cause some kind of regularization.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    \n",
    "The difference is in the amount of samples used to calculate the gradient in each step.\n",
    "In regular gradient decent we use all of our training samples in each step, that may cause the model to overfit and also be to expensive to calculate, or even impossible due to memory.\n",
    "In stochastic gradient decent we use one sample in each step, that may prevent the model from converging to a minimum because the updates are chaotic. \n",
    "In batch gradient decent we use a fixed small number (usualy 2-1024) of samples in each step.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "\n",
    "  i. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "\n",
    "  ii. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Two reasons: \n",
    "\n",
    "i.\n",
    "\n",
    "a. Calculating the gradients for all the samples may take long time, so each gradient step would\n",
    "take long time and the model would train slow\n",
    "\n",
    "b. GD tends to overfit the training set while when using SGD the model may have better generalization.\n",
    "\n",
    "c. SGD & mini batch SGD add stochasticity to the equation, and in non-convex optimization problems which is the common case, this kind of noise helps avoiding\n",
    "local minimums.\n",
    "\n",
    "ii.\n",
    "GD can't be used at all when the data is very large so the values of the gradients can't be all together in the memory at the same time.\n",
    "  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We would expect less iterations to converge. Since we are using more samples in each update steps, the update direction would be more accurate and less stochastic, therfore we need less steps to converge since the optimizer moves to a better direction. Altough we would need less steps to converge, it will not necessarily take less time, because at each iteration we have to use more samples to calculate the loss.\n",
    "Morover, increasing the batch size usually leads to a decrease in the variance of the gradient estimate, which could lead to faster convergence.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. False, in SGD we sample randomly from the training set at each step, some samples may be sampled twice or more in that proccess while other are not sampled at all\n",
    "\n",
    "2. False, gradients in SGD are only affected by one sample. samples may be different therefore the variance will be higher and convergence will be slower\n",
    "\n",
    "3. True, since in SGD different samples are used in each step, new samples may get the optimizer out of a local minima that caused by other samples\n",
    "\n",
    "4. False, in SGD we don't have to use all samples simultaneously( we actually use only 1 or a few in mini-batch SGD case )\n",
    ", so we dont have to keep them all in the memory at the same time\n",
    "\n",
    "5. False, GD is not guaranteed to converge to a global minimum. it depends mainly on the convexity of the loss and somehow in the  initialization.\n",
    "\n",
    "6. False, in Newton's method we will have to compute the second order derivatives for the hessian which might be computationaly heavy, but\n",
    " because of the narrow ravine with high curvature , Newton's method will use that information better (it takes curvature into account) and will eventually converge faster than\n",
    " SGD with momentum.\n",
    " SGD with momentum uses a first order estimation ,will be a faster to compute but probably would converge in more iterations.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "\n",
    "**True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "False, The inner optimization problem can be solved as a constraint in the network's loss function. Therefore we can minimize an expression $L = f(x) + \\lambda g(x)$ without using a descent based method just like we find a minimum on a regular function.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. In vanishing gradients, the gradients goes smaller as they propagate through the network until they get close to zero and act like a numeric zero (chain rule makes low gradients being multiplied one in another and that way decrease even more).\n",
    " In exploding gradients, the gradients grow as they propagate until they get to big, thus the update step is to large so the optimizer can't find the minimum. Here the same thing happens when the gradients multiplied by each other and cause an 'explosion'.\n",
    "\n",
    "2. When back propagating the gradient, we multiply the gradient of the previous layer with the gradient of the current layer (according to the chain rule). Increasing the number of layer, will increase the number of multiplications. For gradients lower then 1, the values exponentially decay to zero, and for gradients higher then 1 the values would exponentially grow (to infinity)\n",
    "\n",
    "3. Consider a network with one unit in each layer, so the function it represents is: $\\hat y = f_n(f_{n-1}(...f_1(x;\\Theta_1)...;\\Theta_{n-1});\\Theta_n))$ for functions $f_i$ parametrized by $\\Theta_i$ as the layers.\n",
    "\n",
    "    for vanishing gradients, let's assume that $\\frac{\\partial L}{ \\partial \\Theta_n} = \\frac{\\partial f_i}{\\partial \\Theta_i} = 0.9$ Which is a little bit lower then 1.\n",
    "    Therefore, $\\frac{\\partial L}{ \\partial \\Theta_1} = \\frac{\\partial L}{ \\partial \\Theta_n} \\frac{\\partial f_{n-1} }{ \\partial \\Theta_{n-1}} ... \\frac{\\partial f_{1} }{ \\partial \\Theta_{1}} = 0.9^n$. We can see an exponential decay with $n$ which is the number of layer. With 1,000 layers for example, we would get a gradient as low as $10^{-46}$ which is practically zero.\n",
    "    \n",
    "    or exploding gradients, let's assume that $\\frac{\\partial L}{ \\partial \\Theta_n} = \\frac{\\partial f_i}{\\partial \\Theta_i} = 1.1$ Which is a little bit higher then 1.\n",
    "    Therefore, $\\frac{\\partial L}{ \\partial \\Theta_1} = \\frac{\\partial L}{ \\partial \\Theta_n} \\frac{\\partial f_{n-1} }{ \\partial \\Theta_{n-1}} ... \\frac{\\partial f_{1} }{ \\partial \\Theta_{1}} = 1.1^n$. We can see an exponential growth with $n$. With 1,000 layers, we would get a gradient as high as $2.47^{41}$ which is practically infinity.\n",
    "    \n",
    "4. We can look at the graph of the loss function. If the loss fluctuates widely, or diverges, it is probably because the gradients are too high, making the optimizer to take too large steps (exploding gradients). If the loss is constant and does not change after a lot of steps (or changes are too small) at early stage of the training, it seems to be a sign for vanishing gradients .\n",
    "The steps are probably too small because the gradient is close to zero.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "first:\n",
    "\n",
    "$\\frac{\\partial L_S}{\\partial \\hat y^{(i)} } = \\frac{1}{N} (- \\frac{y^{(i)}}{\\hat y^{(i)} } + \\frac{1-y^{(i)}}{1- \\hat y^{(i)} }) \n",
    "\\Rightarrow\n",
    "\\frac{\\partial L_S}{\\partial \\hat y } = \\frac{1}{N} (- \\frac{y}{\\hat y } + \\frac{1-y}{1- \\hat y })$ \n",
    "\n",
    "\n",
    "$\\delta W_2 =\n",
    "\\frac{1}{N} \\sum_{i=1}^N {\\frac{\\partial Ls}{\\partial \\hat y^{(i)}} \\frac{\\partial \\hat y^{(i)}}{W_1}} + \\lambda W_2 =\n",
    "\\frac{1}{N^2} \\sum_{i=1}^N {(- \\frac{y^{(i)}}{\\hat y^{(i)}} + \\frac{1-y^{(i)}}{1- \\hat y^{(i)}}) \\cdot \\phi ' x^{(i)} } + \\lambda W_2 $\n",
    "\n",
    "$\\delta W_1 =\n",
    "\\frac{1}{N} \\sum_{i=1}^N {\\frac{\\partial Ls}{\\partial \\hat y^{(i)}} \\frac{\\partial \\hat y^{(i)}}{W_2}} + \\lambda W_1 =\n",
    "\\frac{1}{N^2} \\sum_{i=1}^N {(- \\frac{y^{(i)}}{\\hat y^{(i)}} + \\frac{1-y^{(i)}}{1- \\hat y^{(i)}}) \\cdot \\phi(W_1 x^{(i)} + b_1)} + \\lambda W_1 $\n",
    "\n",
    "$\\delta b_2 = \n",
    "\\frac{1}{N} \\sum_{i=1}^N {\\frac{\\partial Ls}{\\partial \\hat y^{(i)} } \\frac{\\partial \\hat y^{(i)} }{b_2} } = \n",
    "\\frac{1}{N^2} \\sum_{i=1}^N {(- \\frac{y^{(i)}}{\\hat y^{(i)}} + \\frac{1-y^{(i)}}{1- \\hat y^{(i)}})}\n",
    "$\n",
    "\n",
    "$\\delta b_1 = \n",
    "\\frac{1}{N} \\sum_{i=1}^N {\\frac{\\partial Ls}{\\partial \\hat y^{(i)} } \\frac{\\partial \\hat y^{(i)} }{b_1} } = \n",
    "\\frac{1}{N^2} \\sum_{i=1}^N {(- \\frac{y^{(i)}}{\\hat y^{(i)}} + \\frac{1-y^{(i)}}{1- \\hat y^{(i)}}) \\cdot \\phi ' W_2}\n",
    "$\n",
    "\n",
    "$\\delta x = \n",
    "\\frac{1}{N} \\sum_{i=1}^N {\\frac{\\partial Ls}{\\partial \\hat y^{(i)} } \\frac{\\partial \\hat y^{(i)} }{\\partial x} } = \n",
    "\\frac{1}{N^2} \\sum_{i=1}^N {(- \\frac{y^{(i)}}{\\hat y^{(i)}} + \\frac{1-y^{(i)}}{1- \\hat y^{(i)}}) \\cdot \\phi ' W_2 W_1}\n",
    "$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Answers:** \n",
    "    \n",
    "* We can choose a $\\Delta x$ small enough, and by using that $\\Delta x$ in the formula without the limit we can get an approximation of the derivative at a certain point.\n",
    "\n",
    "* The drawbacks are:\n",
    "    - Numeric approach - This is only an approximation of the formula, it can be inaccurate if we choose $\\Delta x$ not small enough or numerically unstable if we choose $\\Delta x$  too small\n",
    "    - Expensive Computationally - It can take long time to use this method to approximate the derivative because we will have to do that w.r.t every parameter at every optimization step.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.8618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "delta_X = 1e-7\n",
    "\n",
    "grad_W = torch.zeros_like(W)\n",
    "for i in range(0,d):\n",
    "    for j in range(0,d):\n",
    "        temp_W = W.clone()\n",
    "        temp_W[i,j] += delta_X\n",
    "        grad_W[i,j] = (foo(temp_W, b) - foo(W,b))/delta_X\n",
    "\n",
    "grad_b = torch.zeros_like(b)\n",
    "for i in range(0,d):\n",
    "    temp_b = b.clone()\n",
    "    temp_b[i] += delta_X\n",
    "    grad_b[i] = (foo(W, temp_b) - foo(W,b))/delta_X\n",
    "\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "# W.zero_grad()\n",
    "# b.zero_grad()\n",
    "foo(W, b)\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "\n",
    "assert torch.allclose(grad_W, autograd_W)\n",
    "assert torch.allclose(grad_b, autograd_b)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Sequence models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** \n",
    "\n",
    "- Word Embeddings encode tokens as tensors in a way that maintain some semantic meaning for our task, meaning that tokens that are \"close\" to each other semantically, will be also close in the embedding space.\n",
    "\n",
    "- Language model can be trained without embedding, using one-hot encoding for the tokens, but it will not work so well. First, the input to the model will have to be much larger - a one-hot vector with a dimension of the number of tokens (actually dictionary size), which can make the model too large and with too much parameters. In addition, the model will take long time to learn without the sentiment of each token.\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4971, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "- Y contains a 42,000 dimensional embedding for each of the tokens (from 43 possible tokens) in the tensor X. The tensor Y should be the same shape of X, but every element in the tensor is an embbeding for a token, so the 42000 is added in the end of the shape.\n",
    "every value in the 4D X tensor has a corresponding embedding vector 42k dimension.\n",
    "- We can create a one-hot representation from each token, then all we have to do is create a mapping from a *num_tokens* dimensional space into an *embedding_dim* dimensional space. it can be easily done with a $(num\\_tokens, embedding\\_dim)$ tensor (or a linear layer).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. True, TBPTT backpropagates through time until a certain past time (defined by some limiting factor).\n",
    "\n",
    "2. False, We need to limit the length of the backpropogation process, this can be done my making sure the gardient accumulation stops after defined number of time steps.\n",
    "3. False, although accumulating gradients stops after S steps, there might exist some information for the far past inside the hidden state\n",
    "and that way the model can learn little relation from the far past.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "\n",
    "  i. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  ii. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "i. The decoder hidden states now have to contain information about the entire input sequence in order to give attention to the correct part. The encoder hidden states is used now as a query to the attention, which means it acts as a feedback to the model itself. All hidden states of the encoder and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in models without attention.\n",
    "\n",
    "ii. We expect now that the hidden states will make the decoder generate output words with consideration to the words that are near them in the sentence, as opposed to implicitly being affected by the attention of the decoder.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answers:**\n",
    "\n",
    "\n",
    "1. we would expect that the images reconstructed by the model during training will be almost identical to the original x,\n",
    "because the loss will focus on the reconstruction term.\n",
    "2. The images generated by the model will be sampling far from N(0,I) distribution since the KL-term ensures that the model posterior and the actual posterior\n",
    "will stay close in some manner. The model will generate garbage in most of the cases since the z is drawn normally (0,I) not coming from encoded x.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answers:**\n",
    "\n",
    "1. False - As we've learnt in the lectures that using the reparameterization trick we achieve z being drawn from $\\mathcal{N}(\\mu_\\alpha(x), \\Sigma_\\alpha(x))$ distribution.\n",
    "2. False - feeding the same image to the encoder every time and decode it, w'ont necessarily produce the same result because the produced images are being drawn from a probability distribution, which is not deterministic.\n",
    "3. True - As we saw in the lecture , we could not compute the actual posterior $p(Z|X)$ and its distance (in KL term) from $q_\\alpha(Z|X)$, so we assume it is very small and we just ignore from it\n",
    "and we are left with lower bound of the log probability. Multiplying both sides with -1 gives us an upper bound of the negative log probability - and that bound is actually what we are trying to minimize.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. False - As we've learnt in the lectures that using the reparameterization trick we achieve z being drawn from $\\mathcal{N}(\\mu_\\alpha(x), \\Sigma_\\alpha(x))$ distribution.\n",
    "2. False - feeding the same image to the encoder every time and decode it, w'ont necessarily produce the same result because the produced images are being drawn from a probability distribution, which is not deterministic.\n",
    "3. True - As we saw in the lecture , we could not compute the actual posterior $p(Z|X)$ and its distance (in KL term) from $q_\\alpha(Z|X)$, so we assume it is very small and we just ignore from it\n",
    "and we are left with lower bound of the log probability. Multiplying both sides with -1 gives us an upper bound of the negative log probability - and that bound is actually what we are trying to minimize.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. False -  ideally we want the generator's loss be low but not too low , so the model could really generate new instances.\n",
    "On the other hand, we want the discriminator's loss to be high (because it means he cant distinguish between real and fake samples)\n",
    "but not too high, because he needs to drive the generator to learn realistic images.\n",
    "2. False - opposed to the generator, training the discriminator does not require backpropagate through the generator.\n",
    "training the discriminator should treat the generated values as constants because it is actually trained like standard classifier.\n",
    "3. True - We've taught our generator to generate images out of noise - $\\mathcal{N}(\\vec{0},\\vec{I})$\n",
    "4. True - Initially training the discriminator for some epochs could help so that the disc's classification wont be out of context at all.\n",
    "and the better the performance of the discriminator is, the generator is challenged better to produce better samples .\n",
    "5. False - if we achieved plausible images and the discriminator reaches stable state of 50% accuracy- it means he can flip a coin to figure\n",
    "if one sample is real or fake. In other words, we reached equilibrium and training more is not likely to improve our model's preformance.\n",
    "The discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its own quality may collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$$\n",
    "  1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "  1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "\n",
    "1.i. As we've learnt in the tutorial , each line of Y contains the feature map extracted by spatial graph convolution of the corresponding input line.\n",
    "\n",
    "ii. This means that vertex number 5 wont be taken into account while calculating the spatial convolution.\n",
    "\n",
    "2. In receptive field for CNN we treated them as the spatial region that the input has an affect on the output neuron.\n",
    "In the case of GCN we can look at the receptive field as receptive paths - since we look at how many nodes away our current node would affect.\n",
    "in GCN we could be interested with learning meaningful receptive paths that contribute mostly to some target node(output).\n",
    "We can see, that the laplacian's power affect significantly on the receptive path of a certain feature map of graph, being said that, the laplacian averages on the  local neighbours - like a CNN kernel ,and\n",
    "thus the cases has a similar concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}