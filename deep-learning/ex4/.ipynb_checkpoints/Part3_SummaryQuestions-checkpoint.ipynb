{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 3: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "The receptive field is the region in the input space that a particular CNN's feature is affected by. Informally, it is the part of a tensor that after convolution results in a feature. So fundamentally, it gives us an idea of where we're getting our results from as data flows through the layers of the network.\n",
    "\n",
    "This is essential in many computer vision tasks. Take, for example, image segmentation. The network takes an input image and predicts the class label of every pixel building a semantic label map in the process. If the network can't take into account enough surrounding pixels when doing its predictions some larger objects might be left with incomplete boundaries.\n",
    "\n",
    "It is an important concept that is very useful to have in mind when designing new models or even trying to understand already existing ones. Knowing about it allows us to further analyze the inner workings of the neural architecture we’re interested in and think about eventual improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "The three different ways to control the rate at which the receptive field grows can be summarized as follows:\n",
    "* Add more convolutional layers (make the network deeper).\n",
    "* Add pooling layers or higher stride convolutions (sub-sampling).\n",
    "* Use dilated convolutions.\n",
    "\n",
    "<i><u>Option 1</u></i> increases the receptive field size <b>linearly</b>, as each extra layer increases the receptive field size by the kernel size. Moreover, it is experimentally validated that as the theoretical receptive field is increasing, however, the effective (experimental) receptive field is reducing. \n",
    "\n",
    "<i><u>Option 2</u></i>, sub-sampling techniques like <u>pooling</u>, increase the receptive field size <b>multiplicatively</b> and in the absence of striding, neighboring nodes in a CNN are influenced by nearly the same set of inputs (i.e., their receptive fields are nearly identical). It is a way to reduce the dimension of the feature map by combining features in the same region. An additional sub-sampling that can be used is the <u>stride</u>, which determines the shift size of the filter, and how large is the overlapping portion of the receptive fields between features.Therefore, the consecutive convolutional layers are affected by increasingly major parts of the input image, which results in a rapid increase in the receptive field size.\n",
    "\n",
    "<i><u>Option 3</u></i>, In essence, dilated convolutions introduce another parameter, denoted as $r$, called the dilation rate. Dilations introduce \"holes\" in a convolutional kernel. The \"holes\" basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of $r$ introduces a kind of striding of $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "Each layer $l$, its spatial configuration is parameterized by these variables below:\n",
    "* <i>k</i>: kernel size (positive integer)\n",
    "* <i>s</i>: stride (positive integer)\n",
    "* <i>p</i>: padding (non-negative integer)\n",
    "* <i>d</i>: dilation (positive integer) \n",
    "\n",
    "The <i><b>Relu Activation</b></i> does not affect the receptive field at all. \\\n",
    "The <i><b>MaxPooling2d(2)</b></i> can be considered as <i>Conv2d</i> with:\n",
    "* kernel size = 2\n",
    "* stride = 2\n",
    "* padding = 0\n",
    "* dilation = 1\n",
    "\n",
    "Define $r$ as the receptive field size of the final output feature map:\n",
    "\n",
    "$r_i = r_{i-1} + d_i(k_i - 1) *  \\prod_{j=1}^{k-1} s_j$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$Input$ \\\n",
    "$r_0 = 1$ \n",
    "\n",
    "$Conv_{k=3, d=1, s=1}$ \\\n",
    "$r_1 = 1 + 1(3 - 1) = 3$ \n",
    "\n",
    "$Max Pooling_{k=2, d=1, s=2}$ \\\n",
    "$r_2 = 3 + 1(2 - 1) * 1 = 4$\n",
    "\n",
    "$Conv_{k=5, d=1, s=2}$ \\\n",
    "$r_3 = 4 + 1(5 - 1) * 1 * 2 = 12$\n",
    "\n",
    "$Max Pooling_{k=2, d=1, s=2}$ \\\n",
    "$r_4 = 12 + 1(2 - 1) * 1 * 2 * 2 = 16$\n",
    "\n",
    "$Conv_{k=7, d=2, s=1}$ \\\n",
    "$r_5 = 16 + 2(7 - 1) * 1 * 2 * 2 * 2 = 112$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "Let $g(x)$ be the original function $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ learned by the layers. \\\n",
    "Let’s consider $h(x)$ be the residual mapping function $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, layers with skip connections (the $+\\vec{x}$ term denotes the skip connection).\n",
    "\n",
    "In the residual mapping function, the $+\\vec{x}$ term will bring the original value, layer $g(x)$ has to learn just the changes in the value, or the residue or <b>delta</b> $x$. \n",
    "Whatever is being learned in $g(x)$ is just the residue, either positive or negative to modify $x$ to the required value, and as a result, the residual network produces completely different filters.\n",
    "\n",
    "For $h(x)$ to be an identity function, the residue $g(x)$ just has to become a zero function, which is very easy to learn, i.e. set all weights to zero. Then $h(x) = 0 + x = x$, which is the required identity function. This will help overcome the degradation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "Dropout works by probabilistically removing, or \"dropping out\" inputs to a layer, which may be input variables in the data sample or activations from a previous layer. Dropping out the elements with probability $p$ will result in keeping the elements with the probability of $1 - p$.\n",
    "\n",
    "Therefore, to replace the two consecutive dropout layers with a single probability, we will keep the elements with the probability of $1 - q$ which is the probability of $1 - p_1$ multiplied by the probability of $1 - p_2$.\n",
    "\n",
    "$1 - q = (1 - p_1) * (1 - p_2)$ \\\n",
    "$1 - q = 1 - p_2 - p_1 + p_1p_2$ \\\n",
    "$q = p_1 + p_2 - p_1p_2$ \\\n",
    "$q = p_1 + p_2(1 - p_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "False.\n",
    "Commonly, dropout is applied after the non-linear activation function. However, when using rectified linear units (ReLUs), it might make sense to apply the dropout before the non-linear activation for reasons of computational efficiency depending on the particular code implementation. Therefore, dropout doesn't necessarily be placed only after the activation function.\n",
    "\n",
    "For example, let's consider the two networks as below:\n",
    "* Network 1: Fully connected, linear activation -> ReLU -> Dropout -> ...\n",
    "* Network 2: Fully connected, linear activation -> Dropout -> ReLU -> ...\n",
    "\n",
    "We will use these outputs of the linear activation of the fully connected layer: $[-1, -2, -3, 4, 5, 6]$\n",
    "\n",
    "Now, applying dropout with a probability of 50%, let’s assume that the units being deactivated are <b>units 2, 4, and 6</b>:\n",
    "* Network 1: $[0, 0, 0, 4, 5, 6] -> [0*2, 0, 0*2, 0, 0*2, 0] = [0, 0, 0, 0, 10, 0]$\n",
    "* Network 2: $[-1, -2, -3, 4, 5, 6] -> [-1*2, 0, -3*2, 0, 5*2, 0] = [0, 0, 0, 0, 10, 0]$\n",
    "\n",
    "As a result, we can observe the same output from both networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "The activations are scaled by $1/(1-p)$ because the expected value of a Dropout network is equivalent to a regular network with its weights scaled with the Dropout rate $p$. The scaling makes the inferences from a Dropout network comparable to the full network (there are computational benefits as well).\n",
    "\n",
    "<b><i>Proof:</i></b>\n",
    "\n",
    "Let's consider $\\hat{x}$ to be the result of the applied activation function on $x$.\n",
    "\n",
    "$\\hat{x} = w \\cdot (1 - p)$, otherwise it will be zero (according to the activation function).\n",
    "\n",
    "$\\mathbb{E}[\\mathbf{\\hat x}] = \\frac{1}{n} \\sum_{i=1}^{n}{\\hat x_i} = \\frac{1}{n} \\sum_{i=1}^{n}{(1-p)x_i} = (1-p) \\mathbb{E}[\\mathbf{x}]$ \n",
    "\n",
    "Therefore, this $(1-p)$ scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "No, we would not train this model with the L2 loss function, instead, a more suitable loss function will be the cross-entropy loss or the logistic loss.\n",
    "\n",
    "There are two reasons why the L2 loss function (i.e. MSE) is an incorrect choice for binary classification problems: \n",
    "* Using the L2 loss function means that we assume that the underlying data has been generated from a normal distribution (a bell-shaped curve). In Bayesian terms this means we assume a Gaussian prior. While in reality, a dataset that can be classified into two categories (i.e binary) is not from a normal distribution but a Bernoulli distribution.\n",
    "* The L2 loss function is non-convex for binary classification. In simple terms, if a binary classification model is trained with the MSE Cost function, it is not guaranteed to minimize the Cost function. This is because the function expects real-valued inputs in range $(-\\infty, \\infty)$, while binary classification models output probabilities in range $(0,1)$ through the sigmoid/logistic function.\n",
    "\n",
    "When the MSE function is passed a value that is unbounded a nice U-shaped (convex) curve is the result where there is a clear minimum point at the target value (y). On the other hand, when a bounded value from a Sigmoid function is passed to the MSE function the result is not convex (on one side the function is concave while on the other side the function is convex and has no clear minimum point). \n",
    "\n",
    "Therefore, if by accident a binary classification neural network is initialized with weights that are large in magnitude such that it lands on the concave part of the MSE Cost function gradient descent will not work and consequently, weights may not update or improve very slowly. This is one of the reasons why neural networks should be carefully initialized with small values when training.\n",
    "\n",
    "To demonstrate this conclusion, if our classifier's output for a hotdog will be 0.65, it means that the model predicted 65% hotdog, hence:\n",
    "\n",
    "The result of the L2 loss function: \\\n",
    "Predicted output = 0.65 \\\n",
    "Actual output = 1\n",
    "\n",
    "$= (1−0.65)^2 = 0.1225$\n",
    "\n",
    "The result of the cross-entropy loss function: \\\n",
    "Predicted output = 0.65 \\\n",
    "Actual output = 1\n",
    "\n",
    "$= −log(0.65)=0.9118$\n",
    "\n",
    "Which indicates a higher probability that this is a hotdog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.Image(\"./imgs/l2-vs-cross.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>In the above image we can present the non-convex for binary classification of the L2 loss function versus the binary classification models output probabilities.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "Due to the depth of the network, which contains many layers (very deep network), and using only the sigmoid activation function (which outputs results in the range of [0,1]), it can cause losing massive information during the training phase. The network reduces many details into a small range.\n",
    "\n",
    "Therefore, it's most likely that this model is suffering from a vanishing gradient, which causes the saturation in the sigmoid function, hence, is seen as a plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the sigmoid activations with the tanh function apparently will not improve this model. \n",
    "\n",
    "As the same as in the sigmoid, the tanh suffers from vanishing gradients as well, since they are very similar, the difference is in their range. \\\n",
    "The tanh's range is [−1,1.] and the sigmoid's range is [0,1]. Thus, it is most likely that the tanh will reach a plateau as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain: \\\n",
    "    a. In a model using exclusively ReLU activations, there can be no vanishing gradients. \\\n",
    "    b. The gradient of ReLU is linear with its input when the input is positive.        \n",
    "    c. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. <b>False.</b> Vanishing gradients are common when the Sigmoid or Tanh activation functions are used in the hidden layer units. When the inputs grow extremely small or extremely large, the sigmoid function saturates at 0 and 1 while the tanh function saturates at -1 and 1. However, ReLU is sometimes used as an activation function to address the vanishing gradient problems, but it can cause the exploding gradients problem (due to getting extremely negative values).\n",
    "\n",
    "b. <b>False.</b> The ReLU activation function is defined as $f(x) = max(0, x)$, which means that the gradient of the ReLU function is 1, hence, the gradient is not linear corresponding to the input.\n",
    "\n",
    "c. <b>True.</b> ReLU can cause \"dead\" neurons\". It is mainly caused by a large negative gradient that flows through the network resulting in a big negative weight for ReLU neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "<b>Gradient Descent:</b> Gradient Descent involves calculations over the full training set at each step as a result of which it is very slow on very large training data. Thus, it becomes very computationally expensive to perform GD. However, this is great for convex or relatively smooth error manifolds. Also, GD scales well with the number of features.\n",
    "\n",
    "<b>Stochastic Gradient Descent:</b> SGD tries to solve the main problem in Gradient descent which is the usage of whole training data to calculate gradients as each step. SGD is stochastic i.e it picks up a \"random\" instance of training data at each step and then computes the gradient making it much faster as there are much fewer data to manipulate at a single time, unlike GD.\n",
    "\n",
    "<b>Mini-batch Stochastic Gradient Descent:</b> mini-batch SGD is a mixture of both Gradient Descent and SGD. Neither do we use the whole dataset all at once nor do we use the single example at a time. We use a batch of a fixed number of training examples that is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD: \\\n",
    "    a. Provide at least two reasons for why SGD is used more often in practice compared to GD. \\\n",
    "    b. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. In both gradient descent (GD) and stochastic gradient descent (SGD), you iteratively update a set of parameters to minimize an error function.\n",
    "\n",
    "While in GD, you have to run through <b>all</b> the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use <b>only one</b> training sample from your training set to do the update for a parameter in a particular iteration.\n",
    "\n",
    "Thus, if the number of training samples is large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.\n",
    "\n",
    "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. \\\n",
    "Often in most cases, the close approximation that you get in SGD for the parameter values is enough because they reach the optimal values and keep oscillating there.\n",
    "\n",
    "b. The cases we prefer to avoid using GD:\n",
    "* It is not suggested for huge training samples (since it is a slow and computationally expensive algorithm and the convergence is slow).\n",
    "* When the loss surface has lots of local minima, thus the GD cannot escape shallow local minima easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "We would expect fewer iterations to converge.\n",
    "\n",
    "Since we would be using more additional samples, the direction of the gradient would be more accurate and less stochastic. Therefore, we need fewer steps to converge due to the optimizer's better direction movements.\n",
    "\n",
    "However, we do obtain fewer steps to converge, but it will not necessarily take us less time, because at each iteration we have to use more samples to calculate the loss. Thus, since the GPUs contain twice the number of RAM uses, we can use vectorization with a larger batch in each iteration so the Time <b>might</b> take to get to that loss value will decrease.\n",
    "\n",
    "Moreover, increasing the batch size could lead to faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why. \\\n",
    "    a. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset. \\\n",
    "    b. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD. \\\n",
    "    c. SGD is less likely to get stuck in local minima, compared to GD. \\\n",
    "    d. Training  with SGD requires more memory than with GD. \\\n",
    "    e. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum. \\\n",
    "    f. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. <b>False.</b> In SGD we use only one training sample from the training set to do the update for a parameter in a particular iteration. \\\n",
    "b. <b>False.</b> SGD computes the gradients based on a single sample from the training set, hence the learning diagram can be extremely noisy and slower than the GD. However, in practice, the time it takes to compute a single sample gradient (instead of the whole dataset) is faster, thus the SGD's convergence is faster than the GD (when training on large datasets). \\\n",
    "c. <b>True.</b> In GD, the value of the cost function decreases gradually, which increases the chance of meeting local minima and makes it impossible to get out of that point. Whereas in SGD, the variation is abrupt. Thus it reduces the probability of getting stuck at local minima, and even if it gets stuck, chances are that it will definitely come out due to its jolting movement. \\\n",
    "d. <b>False.</b> SGD uses only one or a subset of the training samples, compared to the GD that uses the whole dataset. It means that the whole dataset doesn't need to be stored in the memory, hence SGD doesn't require more memory than the GD. \\\n",
    "e. <b>False.</b> both GD and SGD are not guaranteed to converge to a global minimum. It mostly relies on the convexity of the loss surface. \\\n",
    "f. <b>False.</b> Gradient descent maximizes a function using knowledge of its derivative. Newton's method, a root-finding algorithm, maximizes a function using knowledge of its second derivative. That can be faster when the second derivative is known and easy to compute such as narrow ravine (the Newton-Raphson algorithm is used in logistic regression), therefore, Newton's method will converge faster than the SGD with momentum. However, in other cases, the analytic expression for the second derivative is often complicated or intractable, requiring a lot of computation. Numerical methods for computing the second derivative also require a lot of computation (if $n$ values are required to compute the first derivative, $n^2$ values are required for the second derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Bonus** (we didn't discuss this at class):  We can use bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "<b>False.</b> It doesn't rely on the inner optimization problem. \n",
    "Therefore we can minimize the loss without using a descent-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$. \\\n",
    "   Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers. \\\n",
    "    a. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\". \\\n",
    "    b. How can each of these problems be caused by increased depth? \\\n",
    "    c. Provide a numerical example demonstrating each. \\\n",
    "    d. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. The term <b>vanishing gradient</b> refers to the fact that in a feedforward network (FFN) the backpropagated error signal typically decreases (or increases) exponentially as a function of the distance from the final layer. <b>Exploding gradients</b> are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training. This has the effect of your model is unstable and unable to learn from your training data.\n",
    "\n",
    "b. This problem happens because of weights, not because of the activation function. Due to high weight values, the derivatives will also be higher so that the new weight varies a lot to the older weight, and the gradient will never converge. So it may result in oscillating around minima and never come to a global minima point.\n",
    "\n",
    "c. <b>Vanishing Gradient:</b> For instance, if the tanh has a gradient in the range (0,1), thus for a network with n layers, the backpropagation computes gradients by the chain rule such that the accumulation will have the effect of multiplying those smallish gradients n times. It will decrease those gradients exponentially and might affect vanishing gradients for the earlier layers. \\\n",
    "<b>Exploding Gradient:</b> For instance, if we have a network with n layers with initial weights that are larger than 1. Therefore, according to the backpropagation, those weights can get exponentially larger which might cause an exploding gradient.\n",
    "\n",
    "d. By following the graph of the loss function. In case of the loss increase, we can assume that we have an exploding gradient, and in case the loss value remains static/decreases very slowly, we can assume that we have a vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "$ {{\\partial \\ell} \\over {\\partial \\hat{y}}} = {{-y} \\over \\hat{y}} + {{1-y} \\over {1 -\\hat{y}}} = {{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} $\n",
    "\n",
    "$\n",
    "{{\\partial L_s} \\over {\\partial \\mat{W}_1}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{W}_1 x+ b_1}} *\n",
    "{{\\partial \\mat{W}_1 x+ b_1} \\over {\\partial \\mat{W}_1}} +\n",
    "\\lambda  \\norm{\\mat{W}_1}_F = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi ' (\\mat{W}_1 x+ b_1) \\cdot \\mat{x}^T + \\lambda  \\norm{\\mat{W}_1} _F\n",
    "$\n",
    "\n",
    "$\n",
    "{{\\partial L_s} \\over {\\partial \\mat{W}_2}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{W}_2}} +\n",
    "\\lambda  \\norm{\\mat{W}_2}_F = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\varphi(\\mat{W}_1 x+ b_1)^T + \\lambda  \\norm{\\mat{W}_2}_F\n",
    "$\n",
    "\n",
    "$\n",
    "{{\\partial L_s} \\over {\\partial b_1}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{W}_1 x+ b_1}} *\n",
    "{{\\partial \\mat{W}_1 x+ b_1} \\over {\\partial b_1}} =\n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi '(\\mat{W}_1 x+ b_1)\n",
    "$\n",
    "\n",
    "$\n",
    "{{\\partial L_s} \\over {\\partial b_2}} =\n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial b_2}} = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}}\n",
    "$\n",
    "\n",
    "$\n",
    "{{\\partial L_s} \\over {\\partial \\mat{x}}} =\n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{W}_1 x+ b_1}} *\n",
    "{{\\partial \\mat{W}_1 x+ b_1} \\over {\\partial \\mat{x}}} = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi ' (\\mat{W}_1 x+ b_1) \\cdot \\mat{W}_1^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "    a. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD). \\\n",
    "    b. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. We can determine a sufficiently small $\\Delta\\vec{x}$, and use it in the specified formula during the update of our weights in the backpropagation phase. \\\n",
    "Therefore, without using the limit operation, we can obtain an approximation of the derivative at a specific point.\n",
    "\n",
    "b. The drawbacks of this approach:\n",
    "* Numerical: When using the derivative formula, it would grant us an approximation of the result, compared to AD that will provide us the accurate result.\n",
    "* Computational Complexity: Using this method to approximate the derivative can take a longer time since we would need to compute all the parameters at every optimization iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet: \\\n",
    "    a. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question. \\\n",
    "    b. Calculate the same derivatives with autograd. \\\n",
    "    c. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.8774, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "grad_W = torch.zeros(W.shape, dtype=dtype)\n",
    "grad_b = torch.zeros(b.shape, dtype=dtype)\n",
    "delta_X = 1e-7\n",
    "\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        delta_W = torch.clone(W)\n",
    "        delta_W[i,j] = W[i,j] + delta_X\n",
    "        grad_W[i,j] = (foo(delta_W, b) - foo(W, b)) / delta_X\n",
    "\n",
    "for i in range(d):\n",
    "    delta_b = torch.clone(b)\n",
    "    delta_b[i] = b[i] + delta_X\n",
    "    grad_b[i] = (foo(W, delta_b) - foo(W, b)) / delta_X\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "\n",
    "assert torch.allclose(grad_W, autograd_W)\n",
    "assert torch.allclose(grad_b, autograd_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings: \\\n",
    "    a. Explain this term and why it's used in the context of a language model. \\\n",
    "    b. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. Word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. It is being used in natural language processing (NLP), such that words that are close in meaning are grouped near to one another in vector space.\n",
    "\n",
    "b. The language model, like the sentiment analysis, can be trained without an embedding, using the one-hot encoding for the tokens, however, it will not perform very well. \\\n",
    "The one-hot encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain: \\\n",
    "    a. What does `Y` contain? why this output shape? \\\n",
    "    b. **Bonus**: How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.077958Z",
     "iopub.status.busy": "2021-01-26T09:18:18.077299Z",
     "iopub.status.idle": "2021-01-26T09:18:18.378980Z",
     "shell.execute_reply": "2021-01-26T09:18:18.379850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. The Embedding holds a Tensor of dimension (vocab_size, vector_size), i.e. of the size of the vocabulary x the dimension of each vector embedding, and a method that does the lookup. Therefore, it means that Y is a tensor that contains the mapping between words with the shape of (5,6,7,8) to 42 embedding of size 42,000 that represents each of those words\n",
    "\n",
    "b. Perhaps we can construct a one-hot encoding representation from each token. Then, for each token, we can implement our lookup mapping from the tokens' size dimensional space into an \"embedding_dim\" dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain. \\\n",
    "    a. TBPTT uses a modified version of the backpropagation algorithm. \\\n",
    "    b. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S. \\\n",
    "    c. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. <b>True.</b> Truncated Backpropagation Through Time or TBPTT, is a modified version of the BPTT training algorithm for recurrent neural networks where the sequence is processed one timestep at a time, and periodically (k1 timesteps) the BPTT update is performed back for a fixed number of timesteps (k2 timesteps). The Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks like LSTMs. \\\n",
    "b. <b>False.</b> A modification of BPTT (TBPTT) is to limit the number of timesteps used on the backward pass and estimate the gradient used to update the weights rather than calculate it fully. \\\n",
    "c. <b>False.</b> Despite the fact that the gradients' computations stop after S steps (timestamps), they still might hold some information in the states during the training phase, which can learn more than the S timestamps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 5 we learned how to use attention to perform alignment between a source and target sequence in machine translation. \\\n",
    "    a. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention? \\\n",
    "    b. After learning that self-attention is gaining popularity thanks to the transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections, like in the tutorial..). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. \\\n",
    "The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights. \\\n",
    "The encoder hidden states are used as a query to the attention, which means it acts as feedback to the model itself. All hidden states, including the encoder and the decoder, are used to generate the context vector.\n",
    "\n",
    "b. The attention mechanism allows the output to focus attention on input whereas producing output, while the <b>self-attention</b> model allows inputs to interact with each other (their neighbors, and self-attention could be restricted to considering only a neighborhood of size $r$). Therefore, we expect that the influence on the learned hidden states will drive the decoder to produce/generate output words with respect to the nearest words in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "\n",
    "    What would be the qualitative effect of this on: \\\n",
    "    a. Images reconstructed by the model during training ($x\\to z \\to x'$)? \\\n",
    "    b. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "VAEs encode their inputs as normal (Gaussian) distributions rather than points. This is where the KL divergence comes in. It is optimal for the distributions of the VAE to be regularized to increase the amount of overlap within the latent space. KL divergence measures this and is added into the loss function.\n",
    "\n",
    "a. We would expect that the images reconstructed by the model during training will be very similar to the original images since their loss values will focus on the reconstruction term. Hence, the model will be overfitted to the training dataset.\n",
    "\n",
    "b. We would expect that the images generated by the model will have poor results since the prediction of the model is simply generating images based on the Normal distribution $\\mathcal{N}(\\vec{0},\\vec{I})$ and not from encoded images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain: \\\n",
    "    a. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$. \\\n",
    "    b. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction. \\\n",
    "    c. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. <b>False.</b> The latent-space distribution generated by the model for a specific input image can be distinguishable since we are using the KL divergence, it can approximate it to the Normal distribution (using the reparameterization trick). \\\n",
    "b. <b>False.</b> feeding the same image to the encoder multiple times, then decoding each result will not necessarily produce the same reconstruction because the produced images are being sampled from a probability distribution, hence it is not deterministic. \\\n",
    "c. <b>True.</b> The real VAE loss term is intractable, thus we do minimize its upper bound, and hope that the bound is tight appropriately that the model will produce good results. We perform this action due to the lack of the actual posterior and its distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain: \\\n",
    "    a. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator. \\\n",
    "    b. It's crucial to backpropagate into the generator when training the discriminator. \\\n",
    "    c. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$. \\\n",
    "    d. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary. \\\n",
    "    e. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "a. <b>False.</b> Ideally, the generator seeks to minimize the log of the inverse probability predicted by the discriminator for fake images. This has the effect of encouraging the generator to generate samples that have a low probability of being fake.\n",
    "Additionally, The discriminator seeks to maximize the probability assigned to real and fake images (described mathematically, the discriminator seeks to maximize the average of the log probability for real images and the log of the inverted probabilities of fake images).\n",
    "However, we do not want the generator's loss values will be extremely low in order for the model to generate good images, and we do not want that the discriminator's loss values will be extremely high, otherwise, he will not differentiate between fake and real images. \\\n",
    "b. <b>False.</b> It is not crucial to backpropagate into the generator when training the discriminator. Unlike the generator, training the discriminator does not require performing the backpropagation step through its training since it is trained as a formal classifier. \\\n",
    "c. <b>True.</b> To generate a new image, we can sample a latent-space vector from N(0, I) according to our training process which generates images out of this distribution. \\\n",
    "d. <b>True.</b> we assume that it can be beneficial for the discriminator to first be trained for a few epochs, Thus, the discriminator will not be overfitted for a particular time instance of the generator. \\\n",
    "e. <b>False.</b> If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), it means, it is stable and reached the point that it cannot tell the difference between a real image to a fake image. Training the generator more will have bad impacts on both the discriminator and the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection and Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the diffrence between IoU and Dice score? what's the diffrance between IoU and mAP?\n",
    "    shortly explain when would you use what evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "<b>These metrics are commonly used in semantic segmentation evaluation.</b>\n",
    "\n",
    "<b>IoU:</b> The IoU is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth. For binary (two classes) or multi-class segmentation, the mean IoU of the image is calculated by taking the IoU of each class and averaging them.\n",
    "\n",
    "<b>Dice Score:</b> The Dice score is two times the area of overlap divided by the total number of pixels in both images. The Dice score is very similar to the IoU. They are positively correlated, meaning if one says model A is better than model B at segmenting an image, then the other will say the same.\n",
    "\n",
    "Although those two metrics are functionally equivalent so the choice between them is arbitrary, however, the issue comes when taking the average score over a set of inferences. Then the difference emerges when quantifying how much worse classifier B is than A for any given case. In general, the IoU metric tends to penalize single instances of bad classification more than the dice score quantitatively even when they can both agree that this one instance is bad. Similar to how L2 can penalize the largest mistakes more than L1, the IoU metric tends to have a \"squaring\" effect on the errors relative to the dice score. Therefore, the dice score tends to measure something closer to average performance, while the IoU score measures something closer to the worst-case performance.\n",
    "\n",
    "<b>mAP:</b> The mean Average Precision (mAP) score is calculated by taking the mean average precision over all classes and/or overall IoU thresholds, depending on different detection challenges that exist. It gives average accuracy of predicted object locations across all object predictions, matched to ground-truth object predictions, and gives each object equal importance. First, it is normally done separately per class so you get an mAP value for each class, and secondly, mAP is done using bounding boxes with a set threshold of overlap to be considered a match, though theoretically could be done with pixel-wise IoU for a finer analysis).\n",
    "\n",
    "The difference between mAP for object detection and instance segmentation is that when calculating overlaps between predictions and ground-truths, one uses the pixel-wise IoU rather than bounding box IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. regarding of YOLO and mask-r-CNN, witch one is one stage detector? describe the RPN outputs and the YOLO output, adress how the network produce the output and the shapes of each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>\n",
    "\n",
    "<b>YOLO</b> is a one-stage detector model that uses neural networks to provide real-time object detection. It refers to a class of object detection models that are one-stage, i.e. models which skip the region proposal stage of two-stage models and run detection directly over a dense sampling of locations. These types of models usually have faster inference (possibly at the cost of performance).\n",
    "\n",
    "<b>Mask R-CNN</b> is a two-stage detector model for instance segmentation, developed on top of Faster R-CNN. Faster R-CNN predicts object class and bounding boxes. Mask R-CNN is an extension of Faster R-CNN with an additional branch for predicting segmentation masks on each Region of Interest (RoI).\n",
    "\n",
    "<b>YOLO Flow and Output:</b>\n",
    "* YOLO first takes an input image.\n",
    "* The framework then divides the input image into grids (e.g., 3 X 3 grid)\n",
    "* Image classification and localization are applied on each grid. YOLO then predicts the bounding boxes and their corresponding class probabilities for objects (if any are found, of course).\n",
    "* For each of the 9 grids (3X3), we will have an eight-dimensional output vector. This output will have a shape of 3 X 3 X 8.\n",
    "\n",
    "<b>RPN:</b> The Region Proposal Network (RPN) is a fully convolutional network, trained end-to-end, that simultaneously predicts object boundaries and object scores at each detection. It is the faster region-based convolutional neural network (Faster R-CNN) is used to decide \"where\" to look to reduce the computational requirements of the overall inference process. The RPN quickly and efficiently scans every location to assess whether further processing needs to be carried out in a given region. It does that by outputting k bounding box proposals each with 2 scores representing probability of object or not at each location.\n",
    "\n",
    "<b>RPN Flow and Output:</b>\n",
    "* An input image goes through the Convolutional Neural Network and its last layer gives the features maps as output.\n",
    "* A sliding window (e.g., 3X3) is run through the feature maps obtained from the last step. For each sliding window, a particular set of anchors are generated but with 3 different aspect ratios and 3 different scales.\n",
    "* Now we’ve 9 Anchor boxes for each position of the feature map. But there might be many boxes which are not having any object in it. So model needs to learn which anchor box could have the our object in it.\n",
    "* Localizing and classifying the anchor box is done by Bounding box Regressor layer and Bounding box Classifier layer.\n",
    "* The original implementation uses 3 scales and 3 aspect ratios, which means k=9. If the final feature map from feature extraction layer has width W and height H , then the total number of anchors generated will be W*H*k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
